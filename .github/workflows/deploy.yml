# .github/workflows/deploy.yml

name: Deploy Full Financial DWH

on:
  push:
    branches: [ main ]
  workflow_dispatch:

env:
  PROJECT_ID: 'dwhfinancial'
  PROJECT_NUMBER: '675312276583'
  REGION_EU: 'europe-west1'
  REGION_US: 'us-east1'
  GCS_BUCKET: 'dwhfinancial-data-feed'
  DRIVE_SECRET_NAME: 'drive-service-account-key'
  INGESTION_TOPIC: 'drive-check-topic'
  TRANSFORM_TOPIC: 'run-bq-transformations-topic'

  # Datasets BigQuery
  BQ_DATASET_RAW: 'dwh_01_raw'
  BQ_DATASET_BRONZE: 'dwh_02_bronze'
  BQ_DATASET_SILVER: 'dwh_03_silver'
  BQ_DATASET_GOLD: 'dwh_04_gold'

  # Tablas
  BQ_TABLE_TRANSACTIONS: 'transactions'
  BQ_TABLE_SUMMARY: 'monthly_summary'

  # Schedulers
  INGESTION_SCHEDULER_JOB: 'financial-ingestion-trigger'
  TRANSFORM_SCHEDULER_JOB: 'financial-transform-trigger'

jobs:
  deploy-and-run:
    name: Deploy Infrastructure and Functions
    runs-on: ubuntu-latest
    permissions:
      contents: 'read'
      id-token: 'write'

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Authenticate to Google Cloud
      uses: google-github-actions/auth@v2
      with:
        workload_identity_provider: 'projects/${{ env.PROJECT_NUMBER }}/locations/global/workloadIdentityPools/github-pool/providers/github-provider'
        service_account: 'github-actions-deployer@${{ env.PROJECT_ID }}.iam.gserviceaccount.com'

    - name: Set up gcloud CLI
      uses: google-github-actions/setup-gcloud@v2

    # 1) Crear Pub/Sub Topics (idempotente)
    - name: Create Pub/Sub Topics
      run: |
        gcloud pubsub topics create ${{ env.INGESTION_TOPIC }} --project=${{ env.PROJECT_ID }} || echo "Topic already exists"
        gcloud pubsub topics create ${{ env.TRANSFORM_TOPIC }} --project=${{ env.PROJECT_ID }} || echo "Topic already exists"

    # 2) Crear BigQuery datasets (antes de desplegar funciones que los usen)
    - name: Create BigQuery Datasets
      run: |
        bq --location=${{ env.REGION_EU }} mk --dataset ${{ env.PROJECT_ID }}:${{ env.BQ_DATASET_RAW }}     || echo "RAW dataset exists"
        bq --location=${{ env.REGION_EU }} mk --dataset ${{ env.PROJECT_ID }}:${{ env.BQ_DATASET_BRONZE }} || echo "BRONZE dataset exists"
        bq --location=${{ env.REGION_EU }} mk --dataset ${{ env.PROJECT_ID }}:${{ env.BQ_DATASET_SILVER }} || echo "SILVER dataset exists"
        bq --location=${{ env.REGION_EU }} mk --dataset ${{ env.PROJECT_ID }}:${{ env.BQ_DATASET_GOLD }}   || echo "GOLD dataset exists"

    # 3) Deploy Cloud Functions
    - name: Deploy Ingestion ETL (Drive to GCS)
      run: |
        gcloud functions deploy etl_drive_to_gcs \
          --gen2 \
          --region=${{ env.REGION_US }} \
          --runtime=python311 \
          --project=${{ env.PROJECT_ID }} \
          --source=./cloud_functions/etl_drive_to_gcs \
          --entry-point=main \
          --trigger-topic=${{ env.INGESTION_TOPIC }} \
          --set-env-vars="GCP_PROJECT=${{ env.PROJECT_ID }},BUCKET_NAME=${{ env.GCS_BUCKET }},DRIVE_SECRET_NAME=${{ env.DRIVE_SECRET_NAME }}"

    - name: Deploy External Table Creator Function (GCS Trigger)
      run: |
        gcloud functions deploy create_external_table \
          --gen2 \
          --region=${{ env.REGION_US }} \
          --runtime=python311 \
          --project=${{ env.PROJECT_ID }} \
          --source=./cloud_functions/create_external_table \
          --entry-point=main \
          --trigger-event=google.cloud.storage.object.v1.finalized \
          --trigger-resource=${{ env.GCS_BUCKET }} \
          --set-env-vars="GCP_PROJECT=${{ env.PROJECT_ID }},BQ_DATASET_RAW=${{ env.BQ_DATASET_RAW }}"

    - name: Deploy Transformation Function (Run BQ SQL)
      run: |
        gcloud functions deploy run_bq_transformations \
          --gen2 \
          --region=${{ env.REGION_EU }} \
          --runtime=python311 \
          --project=${{ env.PROJECT_ID }} \
          --source=./cloud_functions/run_bq_transformations \
          --entry-point=main \
          --trigger-topic=${{ env.TRANSFORM_TOPIC }} \
          --set-env-vars="GCP_PROJECT=${{ env.PROJECT_ID }},BQ_DATASET_RAW=${{ env.BQ_DATASET_RAW }},BQ_DATASET_BRONZE=${{ env.BQ_DATASET_BRONZE }},BQ_DATASET_SILVER=${{ env.BQ_DATASET_SILVER }},BQ_DATASET_GOLD=${{ env.BQ_DATASET_GOLD }},BQ_TABLE_TRANSACTIONS=${{ env.BQ_TABLE_TRANSACTIONS }},BQ_TABLE_SUMMARY=${{ env.BQ_TABLE_SUMMARY }}"

    # 4) Crear / actualizar schedulers
    - name: Create or Update Cloud Scheduler jobs
      run: |
        gcloud scheduler jobs create pubsub ${{ env.INGESTION_SCHEDULER_JOB }} \
          --schedule="0 5 * * *" \
          --topic=${{ env.INGESTION_TOPIC }} \
          --message-body="Run Ingestion" \
          --location=${{ env.REGION_EU }} \
          --project=${{ env.PROJECT_ID }} \
          --time-zone="Europe/Madrid" \
          --description="Dispara la ingesta de datos desde Drive." \
          || \
        gcloud scheduler jobs update pubsub ${{ env.INGESTION_SCHEDULER_JOB }} \
          --schedule="0 5 * * *" \
          --topic=${{ env.INGESTION_TOPIC }} \
          --message-body="Run Ingestion" \
          --location=${{ env.REGION_EU }} \
          --project=${{ env.PROJECT_ID }} \
          --time-zone="Europe/Madrid"

        gcloud scheduler jobs create pubsub ${{ env.TRANSFORM_SCHEDULER_JOB }} \
          --schedule="0 6 * * *" \
          --topic=${{ env.TRANSFORM_TOPIC }} \
          --message-body="Run Transformation" \
          --location=${{ env.REGION_EU }} \
          --project=${{ env.PROJECT_ID }} \
          --time-zone="Europe/Madrid" \
          --description="Dispara las transformaciones de BigQuery." \
          || \
        gcloud scheduler jobs update pubsub ${{ env.TRANSFORM_SCHEDULER_JOB }} \
          --schedule="0 6 * * *" \
          --topic=${{ env.TRANSFORM_TOPIC }} \
          --message-body="Run Transformation" \
          --location=${{ env.REGION_EU }} \
          --project=${{ env.PROJECT_ID }} \
          --time-zone="Europe/Madrid"
